# Learning Objectives:

- Understand data acquisition: downloading from static links, crawling through entire websites, and streaming data from real-time sources
- Understand data management: organizing data directories

## Clear environment:
```{r}
rm (list = ls())
```

## Where am I?

```{r}
getwd()
```

## Data Acquisition

We are looking at a list of academic institutions from the Carnegie Classifications. These academic institutions share the same characteristics

- Graduate Instructional Program = "Research Doctoral: Highest Research Activity" 
- Enrollment Profile = "High undergraduate" 
- Basic = "Doctoral Universities" 
- Level = "4-year or above"

The list of institutions is available at https://raw.githubusercontent.com/clemsonciti/data-mining-r-workshop/master/institutions.txt

## Create data directory ##

```{r}
current_dir <- getwd()
data_dir <- 'data'

if (!file.exists(data_dir)){
    dir.create(file.path(current_dir, data_dir))
} else {
    print ("Directory already exists")
}
```

## Download data from static link ##

```{r}
institutions <- file.path(current_dir, data_dir, "institutions.csv")
institutions_url <- 'https://raw.githubusercontent.com/clemsonciti/data-mining-r-workshop/master/institutions.txt'

download.file(institutions_url, institutions)
```

```{r}
df_institutions <- read.csv(institutions)
```

```{r}
head(df_institutions)
```

*Automate the process with multiple static links*

Most website's itemized contents (i.e., pages, list of items, etc.) are designed with listing patterns in mind. By identifying and extracting these patterns, it is possible to automate the data acquisition process. 

 Example 1:
 
- https://ncsesdata.nsf.gov/profiles/site?method=view&fice=3963
- https://ncsesdata.nsf.gov/profiles/site?method=view&fice=1081

Example 2: 

- https://www.yelp.com/biz/emerils-new-orleans-new-orleans
- https://www.yelp.com/biz/emerils-new-orleans-new-orleans?start=20
- https://www.yelp.com/biz/emerils-new-orleans-new-orleans?start=40
- https://www.yelp.com/biz/emerils-new-orleans-new-orleans?start=60
- ....

*Data Management*

You are about to download a bunch of files to your workstation. How are you going to organize the new data files?

- Directory and file's names should provide metadata knowledge and serve as an easy way to categorize contents
- Spaces and special characters in names should be avoided at all cost

*Names including data sources: lengthy but meaningful and maintainable*

```{r}
ncses_institution_profiles_dir <- 'ncses_institution_profiles'

if (!file.exists(file.path(data_dir, ncses_institution_profiles_dir))){
    dir.create(file.path(data_dir, ncses_institution_profiles_dir))
    print ('Create directory ncses_institution_profiles under data')
} else {
    print ("Directory already exists")
}
```


```{r}
url_prefix <- 'https://ncsesdata.nsf.gov/profiles/site?method=view&fice='
for (i in 1:nrow(df_institutions)){
    full_url <- paste(url_prefix, df_institutions$FICE[i],sep='')
    print (full_url)
}
```

The previous links are just HTML pages. We need to identify the download URL of the data files. For example:
- https://ncsesdata.nsf.gov/profiles/site?method=download&fice=1081

*How to name your downloaded file? Institution Name or FICE?*

- Why not BOTH?
- How to name your files' extension?

```{r}

for (i in 1:nrow(df_institutions)){    
    institution <- paste(df_institutions$FICE[i],df_institutions$Institutions[i],'.zip',sep='_')
    print(institution)
}
```

*Spaces in the file names! Spaces in the file names!*

```{r}

for (i in 1:nrow(df_institutions)){    
    institution <- paste(df_institutions$FICE[i],df_institutions$Institutions[i],sep='_')
    institution <- gsub(" ", "_", institution, fixed = TRUE)
    institution <- paste(institution,'.zip',sep='')
    print(institution)
}
```

** Challenge **

We will be downloading using Github files instead, as we do not want to crash nsf.gov. We store the institutions' profile data at
https://github.com/clemsonciti/data-mining-r-workshop/tree/master/samples/ncses_institution_profiles

1. What are the URLs to manually download profiles of 2221, 3594, and 8807?



2. What is the corresponding URL prefix that is common for all profiles?



3. Which data inside your R environment that cabe be used to construct the corresponding full URL



*We are really downloading the files now, and also cleaning up the file names*

```{r}
url_prefix <- '_____________________________________'

for (i in 1:nrow(df_institutions)){
    full_url <- paste(url_prefix, ____________, '.zip', sep='')
    
    institution <- paste(df_institutions$FICE[i],df_institutions$Institutions[i],sep='_')
    institution <- gsub(" ", "_", institution, fixed = TRUE)
    institution <- paste(institution,'.zip',sep='')
    institution_path <- file.path(current_dir,data_dir, ncses_institution_profiles_dir, institution)
    print(full_url)
    print(institution_path)
    
    download.file(_____, ______________, mode = "wb")
    
    # be courteous to your source:
    sleep_time <- sample(2:6,1)
    print (sleep_time)
    Sys.sleep(sleep_time)
}
```