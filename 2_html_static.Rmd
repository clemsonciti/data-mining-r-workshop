# Learning Objectives

- Understand data acquisition: crawling through entire websites
- Understand data curation: working with hierarchically structured data (HTML)

## Clear environment

```{r}
rm(list = ls())
```

## Where am I?

```{r}
getwd()
```

We will be using Yelp in this module:

Recaling Yelp's URL patterns from notebook 1:
- https://www.yelp.com/biz/emerils-new-orleans-new-orleans
- https://www.yelp.com/biz/emerils-new-orleans-new-orleans?start=20
- https://www.yelp.com/biz/emerils-new-orleans-new-orleans?start=40
- https://www.yelp.com/biz/emerils-new-orleans-new-orleans?start=60
- ...

**Important:** We do not know when the additional pages will stop. We could go to the last page, but that only works for sometimes, as there will be more reviews in the future. 

We need to wrap the data mining process in a loop whose stopping condition is Yelp running out of further review pages. First step is to analyze a single review page.

**What we want:**
- Information associated with individual reviews (user name, rating, review's text, date ...)
- Information about link to the next review page. 


```{r}
library(xml2)
```


```{r}
url_prefix <- 'https://www.yelp.com/biz/emerils-new-orleans-new-orleans'

current_review <- read_html(url_prefix)
print (current_review)
```


```{r}
xml_structure(current_review)
```

In many cases, looking at just the structure of an HTML page does not help, because you cannot associate the structure's names with the actual relevant contents. Looking at the source of the page can yield better results

We will need to use [XPath Query Language](https://en.wikipedia.org/wiki/XPath):
- /node = top-level node
- //node = node at any level
- node[@attr] = node that has an attribute named "attr"
- node[@attr='something'] = node that has an attribute named "attr" with value 'something'
- node/@attr = value of attribute `attr` in node that has such attributes. 

XPAth queries can be used with package XML's functions to describe operations on invidual XML data elements:


```{r}
single_rev <- xml_find_first(current_review, "//div[@itemprop='review']")
```


```{r}
single_rev
```


```{r}
current_revs <- xml_find_all(current_review, "//div[@itemprop='review']")
```


```{r}
print (length(current_revs))
```

This sounds about right. Now we can examine the internal structure of a single review data element


```{r}
xml_structure(single_rev)
```

List might be a better choice ...


```{r}
list_rev <- as_list(single_rev)
```


```{r}
str(list_rev)
```

How do we get what we need?


```{r}
attr(list_rev[[2]],'content') # Author
```


```{r}
attr(list_rev[[5]],'content') # Review Date
```


```{r}
attr(list_rev[[3]][[2]],'content') # Review Rating
```


```{r}
list_rev[[6]][[1]]
```


```{r}
str(as_list(current_revs))
```

The above shows us the potential structure for a data frame's headers, and for an SQL table's column information. 

Next, we will need to look at stopping conditions when crawling through all the remaining review pages


```{r}
page_info <- xml_find_first(current_review, "//div[@class='page-of-pages arrange_unit arrange_unit--fill']")
```


```{r}
str(as_list(page_info))
```

How to extract information:
- Drop new-line character
- Remove leading and trailing white spaces
- Extract the final number


```{r}
text_page_count <- xml_text(page_info, trim=TRUE)
print (text_page_count)
```


```{r}
page_count <- as.numeric(strsplit(text_page_count," ")[[1]][4])
print (page_count)
```

Let's start the crawling process:
- To test the crawling process, we first try this out by mining the list of reviewers' names


```{r}
url_prefix <- 'https://www.yelp.com/biz/emerils-new-orleans-new-orleans'
url_suffix <- ''
start_index <- 0
list_authors <- c()

for (i in 1:page_count){
    url_current <- paste(url_prefix, url_suffix, sep='')
    current_page <- read_html(url_current)
    current_revs <- xml_find_all(current_page, "//div[@itemprop='review']")
    list_revs <- as_list(current_revs)
    count_revs <- length(list_revs)
    print (url_current)
    print (paste0('Page ', i, ' has ', count_revs, ' reviews.'))
    for (j in 1:count_revs){
        author_name <- attr(list_revs[[j]][[2]],'content')
        list_authors <- c(list_authors, author_name)
    }
    start_index <- start_index + 20
    url_suffix <- paste0('?start=',start_index)
    
    # be courteous to your source:
    sleep_time <- sample(2:6,1)
    print (sleep_time)
    Sys.sleep(sleep_time)
}
print (unique(list_authors))
```


** Challenge **

Mine data for The Smokin Pig (6630 Hwy 76, Pendleton SC) and answer the following questions:

- How many reviews are there into total for this restaurant?
- What is the average rating for The Smokin Pig?


```{r}
url_prefix <- '____________________________'
url_suffix <- ''
start_index <- 0
rating_sum <- 0
rating_count <- 0

current_review <- read_html(url_prefix)
page_info <- xml_find_first(current_review, "//div[@class='page-of-pages arrange_unit arrange_unit--fill']")
text_page_count <- xml_text(page_info, trim=TRUE)
page_count <- as.numeric(strsplit(text_page_count," ")[[1]][4])

for (i in 1:___){
    url_current <- paste(url_prefix, url_suffix, sep='')
    current_page <- read_html(url_current)
    current_revs <- xml_find_all(current_page, "//div[@itemprop='review']")
    
    list_revs <- as_list(current_revs)
    count_revs <- length(list_revs)
    print (url_current)
    print (paste0('Page ', i, ' has ', count_revs, ' reviews.'))
    
    for (j in 1:count_revs){
        rating <- attr(list_revs[[j]][[___]][[___]],'content')
        rating_sum <- ______
        rating_count <- rating_count + 1
    }
    start_index <- start_index + 20
    url_suffix <- paste0('?start=',start_index)
    
    # be courteous to your source:
    sleep_time <- sample(2:6,1)
    print (sleep_time)
    Sys.sleep(sleep_time)
}

avg_rating <- ______
print (paste0('Average Rating: ',avg_rating))
```
