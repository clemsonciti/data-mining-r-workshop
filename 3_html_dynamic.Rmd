# Learning Objectives

- Understand data acquisition: crawling websites with delayed response data


## Clear environment

```{r}
rm(list = ls())
```

## Where am I?

```{r}
getwd()
```

## Dynamic Web Sites

Let's look at the example of mining a search page, in this case, `my.clemson.edu`. Using skills from the previous module, I can do the following:

Set up output directory:

```{r}
current_dir <- getwd()
data_dir <- 'data'
dynamic_data_dir <- 'dynamic_data'

if (!file.exists(file.path(data_dir, dynamic_data_dir))){
    dir.create(file.path(data_dir, dynamic_data_dir))
    print ('Create directory dynamic_data under data')
} else {
    print ("Directory already exists")
}
```

Download data into this directory:

```{r}
library(rvest)

url_prefix <- 'https://my.clemson.edu/#/directory/search/'
search_id <- 'lngo'

search_url <- paste0(url_prefix, search_id)
search_results <- read_html(search_url)

current_results <- xml_find_all(search_results, '//ul[@class="resultsList"]')
print (length(current_results))
```

Further examination shows that we never really have the data:

```{r}
write_xml(search_results, file=file.path(data_dir, 
                                         dynamic_data_dir,
                                         paste0(search_id, '.html')))
```


## Headless browser:

Emulate a web browser to avoid being seen as a 'robot' by websites. This enables the crawler to fully load all
relevant data. 

```{r}
url_prefix <- "https://my.clemson.edu/#/directory/search/"
search_id <- 'lngo'
search_url <- paste0(url_prefix, search_id)

js_dir <- 'js_data'

if (!file.exists(file.path(data_dir, js_dir))){
    dir.create(file.path(data_dir, js_dir))
    print ('Create directory js_data under data')
} else {
    print ("Directory already exists")
}
js_output <- file.path(data_dir,
                       js_dir,
                       paste0(search_id,'_scrape_delayed.js'))

search_results <- file.path(data_dir,
                            dynamic_data_dir,
                            paste0(search_id,'.html'))

lines <- readLines(file.path('template','scrape_delayed.js'))
lines[1] <- paste0("var url ='", search_url, "';")
lines[2] <- paste0("var outFile ='", search_results,"';")
writeLines(lines, js_output)

## Download website
system(paste0("bin/phantomjs ", js_output))
```

** Challenge **

Examine the following page: https://my.clemson.edu/#/directory/advanced-search and answer the following questions:
- How many majors do Clemson University offer in total?

```{r}

```
